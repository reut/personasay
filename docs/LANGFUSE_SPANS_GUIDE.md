# Langfuse Spans Guide - Tracking Persona Flow

This guide explains how to view and analyze the detailed step-by-step flow of each persona's thinking process in Langfuse.

---

## What Are Spans?

**Spans** are sub-steps within a trace. They let you see exactly what happened at each stage of the persona's response generation.

### Persona Response Flow

Each persona response now includes **6 tracked spans**:

```
Trace: Alex responds to "What do you think about this feature?"
â”‚
â”œâ”€ Span 1: load_memory (0.1s)
â”‚  â”œâ”€ Input: {persona: "Alex", session_id: "abc-123"}
â”‚  â””â”€ Output: {messages_loaded: 6, has_history: true}
â”‚
â”œâ”€ Span 2: create_system_prompt (0.05s)
â”‚  â”œâ”€ Input: {persona_role: "Head of Trading", context_mode: "evaluation"}
â”‚  â””â”€ Output: {prompt_length: 2450}
â”‚
â”œâ”€ Span 3: build_messages (0.02s)
â”‚  â”œâ”€ Input: {user_message_length: 45, has_history: true}
â”‚  â””â”€ Output: {total_messages: 2}
â”‚
â”œâ”€ Span 4: llm_call (2.3s) â† Main OpenAI call
â”‚  â”œâ”€ Input: Full prompt with system + user messages
â”‚  â”œâ”€ Output: Persona response text
â”‚  â”œâ”€ Tokens: 450 (input: 200, output: 250)
â”‚  â””â”€ Cost: $0.0045
â”‚
â”œâ”€ Span 5: validate_response (0.08s)
â”‚  â”œâ”€ Input: {response_length: 1250, word_count: 215}
â”‚  â””â”€ Output: {score: 92, passed: true, issues: []}
â”‚
â””â”€ Span 6: save_memory (0.05s)
   â”œâ”€ Input: {session_id: "abc-123", persona: "Alex"}
   â””â”€ Output: {saved: true}
```

**Total Time: 2.6s**

---

## Viewing Spans in Langfuse Dashboard

### Step 1: Go to Traces

1. Open https://cloud.langfuse.com
2. Select your project
3. Click **Traces** in the left sidebar

### Step 2: Select a Trace

Click on any trace to open the detail view.

### Step 3: View the Timeline

You'll see a **Timeline** tab showing:
- All spans in chronological order
- Duration of each span
- Visual timeline bar showing relative time

### Step 4: Inspect Individual Spans

Click on any span to see:
- **Input**: What data went into this step
- **Output**: What came out of this step
- **Duration**: How long it took
- **Metadata**: Additional context

---

## What Each Span Tracks

### 1. load_memory

**Purpose**: Load conversation history from database

**Input:**
```json
{
  "persona": "Alex",
  "session_id": "abc-123"
}
```

**Output:**
```json
{
  "messages_loaded": 6,
  "has_history": true
}
```

**What to look for:**
- High `messages_loaded` = long conversation (good for context, but uses more tokens)
- `has_history: false` = first message in conversation

---

### 2. create_system_prompt

**Purpose**: Generate the persona's system prompt with empathy map, role context, and instructions

**Input:**
```json
{
  "persona_role": "Head of Trading",
  "context_mode": "evaluation"
}
```

**Output:**
```json
{
  "prompt_length": 2450
}
```

**What to look for:**
- `prompt_length` shows how much context the persona has
- Longer prompts = more detailed persona, but higher token cost

---

### 3. build_messages

**Purpose**: Construct the final message array for OpenAI

**Input:**
```json
{
  "user_message_length": 45,
  "has_history": true
}
```

**Output:**
```json
{
  "total_messages": 2
}
```

**What to look for:**
- `total_messages` is typically 2 (system + user)
- If you add chat history to messages, this would be higher

---

### 4. llm_call (Generated by LangChain Callback)

**Purpose**: The actual OpenAI API call

**This span is automatically created by the Langfuse callback handler and includes:**
- Full prompt text
- Complete response
- Token counts (input + output)
- Cost calculation
- Model used (gpt-4o)

**What to look for:**
- **High latency**: Indicates slow OpenAI response (network or load)
- **High token count**: Indicates verbose prompts or responses
- **High cost**: Identify expensive personas

---

### 5. validate_response

**Purpose**: Run quality validation on the response

**Input:**
```json
{
  "response_length": 1250,
  "word_count": 215
}
```

**Output:**
```json
{
  "score": 92,
  "passed": true,
  "issues": []
}
```

**What to look for:**
- `score < 70` = quality issues
- `passed: false` = validation failed
- `issues` array shows specific problems

---

### 6. save_memory

**Purpose**: Persist conversation to database

**Input:**
```json
{
  "session_id": "abc-123",
  "persona": "Alex"
}
```

**Output:**
```json
{
  "saved": true
}
```

**What to look for:**
- `saved: false` would indicate database errors
- Check duration - slow saves indicate database performance issues

---

## Quality Scores

In addition to spans, each trace includes **5 automated quality scores**:

### 1. response_length
- Score: 0.0 - 1.0
- Optimal: 150-250 words = 1.0
- Too short (<100) or too long (>300) = 0.5

### 2. empathy_compliance
- Score: 0.0 - 1.0
- Checks for empathy framework keywords
- 3+ dimensions present = 1.0

### 3. specificity
- Score: 0.0 - 1.0
- Measures concrete examples vs generic statements
- Numbers, examples, specific names = higher score

### 4. role_consistency
- Score: 0.0 - 1.0
- Validates role-specific terminology
- Trading role using trading terms = higher score

### 5. overall_quality
- Score: 0.0 - 1.0
- Weighted average of all evaluators
- 0.9+ = excellent, 0.75+ = good, 0.6+ = acceptable

---

## Analyzing Persona Performance

### Query 1: Which Persona is Fastest?

**Dashboard â†’ Traces:**
1. Group by: `metadata.persona_name`
2. Aggregate: Average of `latency`
3. Sort: Ascending

**Result:** See which personas respond fastest.

---

### Query 2: Which Persona Has Best Quality?

**Dashboard â†’ Scores:**
1. Filter: `score_name = "overall_quality"`
2. Group by: `metadata.persona_name`
3. Aggregate: Average of `value`
4. Sort: Descending

**Result:** See which personas have highest quality scores.

---

### Query 3: Cost Per Persona

**Dashboard â†’ Traces:**
1. Group by: `metadata.persona_id`
2. Aggregate: Sum of `cost`
3. Chart: Bar chart

**Result:** See total cost per persona.

---

### Query 4: Bottleneck Analysis

**Select a trace â†’ Timeline tab:**

Look at the span durations:
- If `llm_call` is 90%+ of total time â†’ OpenAI latency (normal)
- If `load_memory` is high â†’ Database performance issue
- If `validate_response` is high â†’ Validation logic issue
- If `save_memory` is high â†’ Database write performance issue

---

## Example: Debugging a Slow Response

**Scenario:** Alex takes 8 seconds to respond (too slow)

**Steps:**
1. Go to Traces â†’ Filter by `metadata.persona_name = "Alex"`
2. Sort by latency descending
3. Click on the slow trace
4. View Timeline tab
5. Check span durations:

**Example findings:**
```
load_memory: 0.1s (normal)
create_system_prompt: 0.05s (normal)
build_messages: 0.02s (normal)
llm_call: 7.5s (SLOW - OpenAI issue)
validate_response: 0.08s (normal)
save_memory: 0.25s (slightly slow)
```

**Diagnosis:** OpenAI API is slow (7.5s). This is external latency, not a PersonaSay issue.

**Action:** Check OpenAI status page or consider using gpt-4o-mini for faster responses.

---

## Example: Debugging Low Quality

**Scenario:** Nina's responses get low user feedback scores

**Steps:**
1. Go to Scores â†’ Filter by `metadata.persona_name = "Nina"`
2. Check `overall_quality` scores
3. Click on a low-scoring trace
4. View the evaluations:

**Example findings:**
```
response_length: 0.5 (too short - only 80 words)
empathy_compliance: 0.6 (weak empathy markers)
specificity: 0.4 (too generic)
role_consistency: 1.0 (good role terms)
overall_quality: 0.6 (needs improvement)
```

**Diagnosis:** Nina's responses are too short and generic.

**Action:** Adjust Nina's system prompt to encourage longer, more specific responses.

---

## Multi-Persona Conversation Flow

When you chat with 3 personas (Alex, Nina, Ben), Langfuse shows:

```
Session: abc-123 (Total: 6.8s, $0.015)
â”‚
â”œâ”€ Trace: Alex (2.3s, $0.005)
â”‚  â”œâ”€ load_memory (0.1s)
â”‚  â”œâ”€ create_system_prompt (0.05s)
â”‚  â”œâ”€ build_messages (0.02s)
â”‚  â”œâ”€ llm_call (2.0s)
â”‚  â”œâ”€ validate_response (0.08s)
â”‚  â””â”€ save_memory (0.05s)
â”‚
â”œâ”€ Trace: Nina (2.1s, $0.0048)
â”‚  â”œâ”€ load_memory (0.09s)
â”‚  â”œâ”€ create_system_prompt (0.04s)
â”‚  â”œâ”€ build_messages (0.02s)
â”‚  â”œâ”€ llm_call (1.9s)
â”‚  â”œâ”€ validate_response (0.03s)
â”‚  â””â”€ save_memory (0.02s)
â”‚
â””â”€ Trace: Ben (2.4s, $0.0052)
   â”œâ”€ load_memory (0.12s)
   â”œâ”€ create_system_prompt (0.05s)
   â”œâ”€ build_messages (0.02s)
   â”œâ”€ llm_call (2.15s)
   â”œâ”€ validate_response (0.04s)
   â””â”€ save_memory (0.02s)
```

**Insights:**
- All 3 personas run in parallel (total time â‰ˆ slowest persona)
- Ben is slightly slower (2.4s vs 2.1s for Nina)
- Alex costs slightly more ($0.005 vs $0.0048)
- All personas have similar span patterns

---

## Debate Flow Tracking

For debates, you'll see multiple rounds grouped by session:

```
Session: debate-xyz (5 rounds, 3 personas, 15 traces total)
â”‚
â”œâ”€ Round 1
â”‚  â”œâ”€ Alex (metadata: {round_number: 1})
â”‚  â”œâ”€ Nina (metadata: {round_number: 1})
â”‚  â””â”€ Ben (metadata: {round_number: 1})
â”‚
â”œâ”€ Round 2
â”‚  â”œâ”€ Alex (metadata: {round_number: 2})
â”‚  â”œâ”€ Nina (metadata: {round_number: 2})
â”‚  â””â”€ Ben (metadata: {round_number: 2})
â”‚
â””â”€ ... (rounds 3-5)
```

**Filter by round:**
```
metadata.round_number = 3
```

---

## Advanced: Custom Span Analysis

### Identify Memory Load Bottlenecks

**Query:**
1. Filter: `span_name = "load_memory"`
2. Filter: `duration > 500` (ms)
3. Sort: Duration descending

**Result:** Find sessions with slow memory loads (indicates large conversation history).

### Track Validation Failures

**Query:**
1. Filter: `span_name = "validate_response"`
2. Filter: `output.passed = false`

**Result:** See all responses that failed quality validation.

### Monitor Database Performance

**Query:**
1. Filter: `span_name = "save_memory"`
2. Aggregate: Average of `duration`
3. Chart: Time series

**Result:** Track database write performance over time.

---

## Exporting Data

### Export Traces for Analysis

1. Go to Traces
2. Apply filters (e.g., last 7 days, specific persona)
3. Click **Export** â†’ CSV
4. Analyze in Excel, Google Sheets, or Python

### Example Analysis in Python

```python
import pandas as pd

# Load exported traces
df = pd.read_csv('langfuse_traces.csv')

# Average latency per persona
persona_latency = df.groupby('metadata.persona_name')['latency'].mean()
print(persona_latency)

# Cost per feature
feature_cost = df.groupby('metadata.feature')['cost'].sum()
print(feature_cost)

# Quality scores per persona
quality = df[df['score_name'] == 'overall_quality'].groupby('metadata.persona_name')['score_value'].mean()
print(quality)
```

---

## Real-Time Monitoring

### Set Up Alerts

**Alert 1: High Cost**
- Condition: Daily cost > $5
- Action: Email notification

**Alert 2: Low Quality**
- Condition: overall_quality < 0.6
- Action: Slack notification

**Alert 3: Slow Response**
- Condition: latency > 10000ms
- Action: Email notification

**How to set up:**
1. Langfuse Dashboard â†’ Settings â†’ Alerts
2. Create new alert
3. Set condition and notification channel

---

## Troubleshooting

### Spans Not Appearing

**Issue:** You see traces but no spans inside them.

**Fix:** Make sure you're using the latest code with span tracking. Check that the backend restarted after the changes.

### Span Errors

**Issue:** Some spans show errors or are missing.

**Fix:** Check backend logs for exceptions during span creation. Span errors are caught and logged but don't break the response.

### Incomplete Spans

**Issue:** A span has input but no output.

**Fix:** This indicates an exception occurred during that step. Check the trace for error messages.

---

## Best Practices

### 1. Use Spans to Optimize

- Identify slow steps and optimize them
- Focus on spans that take >10% of total time
- Ignore spans <50ms unless they're in a hot path

### 2. Monitor Span Patterns

- Consistent span durations = healthy system
- Sudden spikes in specific spans = investigate
- Missing spans = code path changed or error occurred

### 3. Compare Personas

- Export span data for all personas
- Compare average durations per span
- Identify outliers (e.g., one persona is always slow)

### 4. Track Over Time

- Monitor span durations weekly
- Set up alerts for anomalies
- Track improvements after optimizations

---

## What You'll Learn

With span tracking, you can answer:

1. **"Why is Alex slower than Nina?"**
   - Compare their `llm_call` span durations
   - Check if Alex has longer prompts (see `create_system_prompt` output)

2. **"Why did this response fail validation?"**
   - Look at `validate_response` span output
   - See specific issues in the output

3. **"Is database performance degrading?"**
   - Track `load_memory` and `save_memory` span durations over time
   - Set alert if average duration increases

4. **"Which step is the bottleneck?"**
   - View timeline to see which span takes most time
   - Usually `llm_call` (OpenAI) is 85-95% of total time

---

## Next Steps

1. **Send a test message** in PersonaSay
2. **Go to Langfuse Dashboard** â†’ Traces
3. **Click on the latest trace**
4. **View Timeline tab** to see all spans
5. **Explore the data** and get familiar with the interface

You now have full visibility into every step of the persona thinking process! ğŸ‰

---

*For more information, see: [LANGFUSE_GUIDE.md](LANGFUSE_GUIDE.md)*
